{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast MTCNN detector\n",
    "\n",
    "This notebook demonstrates how to achieve 45 frames per second speeds for loading frames and detecting faces on full resolution videos.\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "**Striding**: The algorithm used is a strided modification of MTCNN in which face detection is performed on only every _N_ frames, and applied to all frames. For example, with a batch of 9 frames, we could pass frames 0, 3, and 6 to MTCNN. Then, the bounding boxes (and potentially landmarks) returned for frame 0 would be naively applied to frames 1 and 2. Similarly, the detections for frame 3 are applied to frames 4 and 5, and the detections for frames 6 are applied to frames 7 and 8.\n",
    "\n",
    "Although this assume that faces do not move between frames significantly, this is generally a good approximation for low stride numbers. If the stride is 3, we are assuming that the face does not significantly alter position for an additional 2 frames, or ~0.07 seconds. If faces are moving faster than this, they are likely to be extremely blurry anyway. Furthermore, ensuring that faces are cropped with a small margin mitigates the impact of face drift.\n",
    "\n",
    "**Scale pyramid**: The algorithm uses a slightly smaller scaling factor (0.6 vs 0.709) than the original MTCNN algorithm to construct the scaling pyramid applied to input images. For details of the scaling pyramid, see the [original paper](https://arxiv.org/abs/1604.02878) for details of the scaling pyramid approach.\n",
    "\n",
    "**Multi-threading**: A modest performance gain comes from loading video frames (with `cv2.VideoCapture`) using threading. This functionality is provided by the `FileVideoStream` class of the imutils package.\n",
    "\n",
    "## Other resources\n",
    "\n",
    "See the following kernel for a guide to using the MTCNN functionality of facenet-pytorch: https://www.kaggle.com/timesler/guide-to-mtcnn-in-facenet-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4328/469605663.py:19: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  timestamped_frames = np.array([[0,tuple()]])\n"
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from PIL import Image\n",
    "import torch\n",
    "from imutils.video import FileVideoStream\n",
    "import cv2\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets,transforms\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "input_video_folder = './videos/*.avi'\n",
    "images_folder = './frames/'\n",
    "target_name = 'divyansh'\n",
    "timestamped_frames = np.array([[0,tuple()]])\n",
    "output_video_name = f\"divyansh-lt-reborn-.85.avi\"\n",
    "output_fps = 23\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Face Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face detected with probability: 0.999979\n",
      "Face detected with probability: 0.999989\n",
      "Face detected with probability: 0.999998\n",
      "Face detected with probability: 0.999979\n",
      "Face detected with probability: 1.000000\n",
      "Face detected with probability: 0.999993\n",
      "Face detected with probability: 0.999815\n",
      "Face detected with probability: 0.999991\n",
      "Face detected with probability: 0.999990\n",
      "Face detected with probability: 0.999997\n",
      "Face detected with probability: 0.999999\n",
      "Face detected with probability: 0.999994\n",
      "Face detected with probability: 1.000000\n",
      "Face detected with probability: 0.999999\n",
      "Face detected with probability: 0.999970\n",
      "Face detected with probability: 0.999997\n",
      "Face detected with probability: 0.999997\n",
      "Face detected with probability: 0.999994\n",
      "Face detected with probability: 0.999992\n",
      "Face detected with probability: 0.999968\n",
      "Face detected with probability: 0.999958\n",
      "Face detected with probability: 0.999973\n",
      "Face detected with probability: 0.999881\n",
      "Face detected with probability: 0.999810\n",
      "Face detected with probability: 0.999861\n",
      "          divyansh  divyansh  divyansh  divyansh  divyansh  divyansh  \\\n",
      "divyansh  1.000000  0.835695  0.887559  0.934867  0.900665  0.887497   \n",
      "divyansh  0.835695  1.000000  0.901821  0.836760  0.869600  0.850042   \n",
      "divyansh  0.887559  0.901821  1.000000  0.872119  0.936130  0.879096   \n",
      "divyansh  0.934867  0.836760  0.872119  1.000000  0.882221  0.876261   \n",
      "divyansh  0.900665  0.869600  0.936130  0.882221  1.000000  0.957632   \n",
      "divyansh  0.887497  0.850042  0.879096  0.876261  0.957632  1.000000   \n",
      "divyansh  0.935725  0.842149  0.846003  0.927172  0.877491  0.877031   \n",
      "divyansh  0.849823  0.879214  0.905510  0.856736  0.846706  0.810714   \n",
      "divyansh  0.801732  0.861083  0.888292  0.804510  0.827266  0.792461   \n",
      "divyansh  0.840757  0.883428  0.905164  0.821095  0.843674  0.805343   \n",
      "divyansh  0.844295  0.865744  0.915836  0.831518  0.875718  0.845272   \n",
      "divyansh  0.848554  0.869399  0.910545  0.840100  0.890608  0.878407   \n",
      "divyansh  0.864250  0.863198  0.914768  0.861257  0.889423  0.876252   \n",
      "divyansh  0.847701  0.863144  0.897305  0.843063  0.897945  0.901944   \n",
      "divyansh  0.858280  0.865368  0.873349  0.828379  0.906213  0.909916   \n",
      "divyansh  0.877777  0.866420  0.892254  0.850237  0.895797  0.897440   \n",
      "divyansh  0.882788  0.854887  0.884245  0.864701  0.874240  0.885155   \n",
      "divyansh  0.868850  0.877055  0.906151  0.851220  0.829479  0.814207   \n",
      "divyansh  0.860021  0.883725  0.880402  0.826460  0.824802  0.825466   \n",
      "divyansh  0.894969  0.871541  0.886388  0.859121  0.849588  0.854524   \n",
      "divyansh  0.880239  0.916452  0.904466  0.862964  0.863966  0.871229   \n",
      "divyansh  0.876469  0.883954  0.851219  0.832513  0.837603  0.831224   \n",
      "divyansh  0.810764  0.860335  0.829752  0.823246  0.782848  0.782534   \n",
      "divyansh  0.690009  0.697055  0.726147  0.698487  0.682118  0.688273   \n",
      "divyansh  0.808864  0.895249  0.885296  0.807304  0.817952  0.786126   \n",
      "\n",
      "          divyansh  divyansh  divyansh  divyansh  ...  divyansh  divyansh  \\\n",
      "divyansh  0.935725  0.849823  0.801732  0.840757  ...  0.877777  0.882788   \n",
      "divyansh  0.842149  0.879214  0.861083  0.883428  ...  0.866420  0.854887   \n",
      "divyansh  0.846003  0.905510  0.888292  0.905164  ...  0.892254  0.884245   \n",
      "divyansh  0.927172  0.856736  0.804510  0.821095  ...  0.850237  0.864701   \n",
      "divyansh  0.877491  0.846706  0.827266  0.843674  ...  0.895797  0.874240   \n",
      "divyansh  0.877031  0.810714  0.792461  0.805343  ...  0.897440  0.885155   \n",
      "divyansh  1.000000  0.801889  0.774627  0.796892  ...  0.841292  0.847787   \n",
      "divyansh  0.801889  1.000000  0.965671  0.975732  ...  0.881508  0.871146   \n",
      "divyansh  0.774627  0.965671  1.000000  0.981175  ...  0.870747  0.861425   \n",
      "divyansh  0.796892  0.975732  0.981175  1.000000  ...  0.890652  0.881029   \n",
      "divyansh  0.800941  0.948305  0.954341  0.959032  ...  0.929784  0.899742   \n",
      "divyansh  0.817085  0.920210  0.928383  0.933363  ...  0.951760  0.922281   \n",
      "divyansh  0.831635  0.921034  0.909769  0.920650  ...  0.955071  0.931209   \n",
      "divyansh  0.816643  0.876656  0.880716  0.889213  ...  0.971757  0.941953   \n",
      "divyansh  0.823794  0.849172  0.837571  0.860468  ...  0.972116  0.946377   \n",
      "divyansh  0.841292  0.881508  0.870747  0.890652  ...  1.000000  0.976472   \n",
      "divyansh  0.847787  0.871146  0.861425  0.881029  ...  0.976472  1.000000   \n",
      "divyansh  0.823332  0.903105  0.859800  0.887125  ...  0.903889  0.921513   \n",
      "divyansh  0.823999  0.886666  0.842565  0.873046  ...  0.911072  0.926509   \n",
      "divyansh  0.867017  0.893862  0.866336  0.889720  ...  0.923142  0.944281   \n",
      "divyansh  0.848023  0.911562  0.895510  0.907186  ...  0.945131  0.947791   \n",
      "divyansh  0.876777  0.829472  0.793162  0.830413  ...  0.899450  0.897458   \n",
      "divyansh  0.807098  0.863795  0.849106  0.857859  ...  0.826010  0.803698   \n",
      "divyansh  0.674901  0.797500  0.805621  0.797366  ...  0.729536  0.726624   \n",
      "divyansh  0.778309  0.959854  0.956586  0.959044  ...  0.891854  0.870815   \n",
      "\n",
      "          divyansh  divyansh  divyansh  divyansh  divyansh  divyansh  \\\n",
      "divyansh  0.868850  0.860021  0.894969  0.880239  0.876469  0.810764   \n",
      "divyansh  0.877055  0.883725  0.871541  0.916452  0.883954  0.860335   \n",
      "divyansh  0.906151  0.880402  0.886388  0.904466  0.851219  0.829752   \n",
      "divyansh  0.851220  0.826460  0.859121  0.862964  0.832513  0.823246   \n",
      "divyansh  0.829479  0.824802  0.849588  0.863966  0.837603  0.782848   \n",
      "divyansh  0.814207  0.825466  0.854524  0.871229  0.831224  0.782534   \n",
      "divyansh  0.823332  0.823999  0.867017  0.848023  0.876777  0.807098   \n",
      "divyansh  0.903105  0.886666  0.893862  0.911562  0.829472  0.863795   \n",
      "divyansh  0.859800  0.842565  0.866336  0.895510  0.793162  0.849106   \n",
      "divyansh  0.887125  0.873046  0.889720  0.907186  0.830413  0.857859   \n",
      "divyansh  0.884438  0.872192  0.885643  0.904521  0.825775  0.837703   \n",
      "divyansh  0.876726  0.867282  0.891504  0.917566  0.852672  0.827145   \n",
      "divyansh  0.898549  0.886658  0.899674  0.924344  0.858290  0.831293   \n",
      "divyansh  0.871193  0.865721  0.874242  0.912913  0.871119  0.840157   \n",
      "divyansh  0.865124  0.879880  0.885141  0.911937  0.886240  0.805647   \n",
      "divyansh  0.903889  0.911072  0.923142  0.945131  0.899450  0.826010   \n",
      "divyansh  0.921513  0.926509  0.944281  0.947791  0.897458  0.803698   \n",
      "divyansh  1.000000  0.978256  0.962046  0.950530  0.901511  0.821244   \n",
      "divyansh  0.978256  1.000000  0.970507  0.954157  0.905228  0.812422   \n",
      "divyansh  0.962046  0.970507  1.000000  0.967313  0.927413  0.830643   \n",
      "divyansh  0.950530  0.954157  0.967313  1.000000  0.926067  0.853606   \n",
      "divyansh  0.901511  0.905228  0.927413  0.926067  1.000000  0.836441   \n",
      "divyansh  0.821244  0.812422  0.830643  0.853606  0.836441  1.000000   \n",
      "divyansh  0.714108  0.700673  0.735426  0.733008  0.711808  0.915552   \n",
      "divyansh  0.885202  0.880364  0.876421  0.901281  0.826763  0.906486   \n",
      "\n",
      "          divyansh  divyansh  \n",
      "divyansh  0.690009  0.808864  \n",
      "divyansh  0.697055  0.895249  \n",
      "divyansh  0.726147  0.885296  \n",
      "divyansh  0.698487  0.807304  \n",
      "divyansh  0.682118  0.817952  \n",
      "divyansh  0.688273  0.786126  \n",
      "divyansh  0.674901  0.778309  \n",
      "divyansh  0.797500  0.959854  \n",
      "divyansh  0.805621  0.956586  \n",
      "divyansh  0.797366  0.959044  \n",
      "divyansh  0.772576  0.936034  \n",
      "divyansh  0.755756  0.911510  \n",
      "divyansh  0.749576  0.907149  \n",
      "divyansh  0.761295  0.885437  \n",
      "divyansh  0.715287  0.854394  \n",
      "divyansh  0.729536  0.891854  \n",
      "divyansh  0.726624  0.870815  \n",
      "divyansh  0.714108  0.885202  \n",
      "divyansh  0.700673  0.880364  \n",
      "divyansh  0.735426  0.876421  \n",
      "divyansh  0.733008  0.901281  \n",
      "divyansh  0.711808  0.826763  \n",
      "divyansh  0.915552  0.906486  \n",
      "divyansh  1.000000  0.828476  \n",
      "divyansh  0.828476  1.000000  \n",
      "\n",
      "[25 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "# # The model is running on CPU, since it is already pre-trained and doesnt require GPU\n",
    "# device = 'cpu'\n",
    "# print('Running on device: {}'.format(device))\n",
    "\n",
    "#Define MTCNN module\n",
    "#Since MTCNN is a collection of neural nets and other code, \n",
    "#The device must be passed in the following way to enable copying of objects when needed internally.\n",
    "mtcnn = MTCNN(\n",
    "    image_size=160, margin=0, min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7], factor=0.709,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "#Function takes 2 vectors 'a' and 'b'\n",
    "#Returns the cosine similarity according to the definition of the dot product\n",
    "def cos_sim(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "#cos_sim returns real numbers,where negative numbers have different interpretations.\n",
    "#So we use this function to return only positive values.\n",
    "def cos(a,b):\n",
    "    minx = -1 \n",
    "    maxx = 1\n",
    "    return (cos_sim(a,b)- minx)/(maxx-minx)\n",
    "\n",
    "# Define Inception Resnet V1 module (GoogLe Net)\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "# Define a dataset and data loader\n",
    "dataset = datasets.ImageFolder(images_folder)\n",
    "dataset.idx_to_class = {i:c for c, i in dataset.class_to_idx.items()}\n",
    "loader = DataLoader(dataset, collate_fn=lambda x: x[0])\n",
    "\n",
    "#Perfom MTCNN facial detection\n",
    "#Detects the face present in the image and prints the probablity of face detected in the image.\n",
    "aligned = []\n",
    "names = []\n",
    "for x, y in loader:\n",
    "    x_aligned, prob = mtcnn(x, return_prob=True)\n",
    "    if x_aligned is not None:\n",
    "        print('Face detected with probability: {:8f}'.format(prob))\n",
    "        aligned.append(x_aligned)\n",
    "        names.append(dataset.idx_to_class[y])\n",
    "\n",
    "# Calculate the 512 face embeddings\n",
    "aligned = torch.stack(aligned).to(device)\n",
    "embeddings = resnet(aligned).cpu()\n",
    "\n",
    "# Print distance matrix for classes.\n",
    "#The embeddings are plotted in space and cosine distace is measured.\n",
    "cos_sim = nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "for i in range(0,len(names)):\n",
    "    emb=embeddings[i].unsqueeze(0)\n",
    "    # The cosine similarity between the embeddings is given by 'dist'.\n",
    "    dist =cos(embeddings[0],emb)  \n",
    "        \n",
    "dists = [[cos(e1,e2).item() for e2 in embeddings] for e1 in embeddings]\n",
    "# The print statement below is\n",
    "#Helpful for analysing the results and for determining the value of threshold.\n",
    "print(pd.DataFrame(dists, columns=names, index=names)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The FastMTCNN class\n",
    "\n",
    "The class below is a thin wrapper for the MTCNN implementation in the `facenet-pytorch` package that implements the algorithm described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastMTCNN(object):\n",
    "    \"\"\"Fast MTCNN implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, stride, resize=1, *args, **kwargs):\n",
    "        \"\"\"Constructor for FastMTCNN class.\n",
    "        \n",
    "        Arguments:\n",
    "            stride (int): The detection stride. Faces will be detected every `stride` frames\n",
    "                and remembered for `stride-1` frames.\n",
    "        \n",
    "        Keyword arguments:\n",
    "            resize (float): Fractional frame scaling. [default: {1}]\n",
    "            *args: Arguments to pass to the MTCNN constructor. See help(MTCNN).\n",
    "            **kwargs: Keyword arguments to pass to the MTCNN constructor. See help(MTCNN).\n",
    "        \"\"\"\n",
    "        self.stride = stride\n",
    "        self.resize = resize\n",
    "        self.mtcnn = MTCNN(*args, **kwargs)\n",
    "        \n",
    "    def __call__(self, frames, do_process):\n",
    "        \"\"\"Detect faces in frames using strided MTCNN.\"\"\"\n",
    "        if self.resize != 1:\n",
    "            frames = [\n",
    "                cv2.resize(f, (int(f.shape[1] * self.resize), int(f.shape[0] * self.resize)))\n",
    "                    for f in frames\n",
    "            ]\n",
    "                      \n",
    "        boxes, probs = self.mtcnn.detect(frames[::self.stride])\n",
    "        \n",
    "        faces = []\n",
    "        for i, frame in enumerate(frames):\n",
    "            # frames not processed here\n",
    "            # if not do_process[i]: \n",
    "            #     continue\n",
    "            box_ind = int(i / self.stride)\n",
    "            if boxes[box_ind] is None:\n",
    "                continue\n",
    "            for box in boxes[box_ind]:\n",
    "                box = [int(b) for b in box]\n",
    "                faces.append(frame[box[1]:box[3], box[0]:box[2]])\n",
    "        \n",
    "        return faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full resolution detection\n",
    "\n",
    "In this example, we demonstrate how to detect faces using full resolution frames (i.e., `resize=1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_mtcnn = FastMTCNN(\n",
    "    image_size=160, min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7],\n",
    "    stride=4,\n",
    "    resize=1,\n",
    "    margin=14,\n",
    "    factor=0.6,\n",
    "    keep_all=True,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognize Faces   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(faces): \n",
    "    for j,l in enumerate(faces):\n",
    "        minDist = float(\"-inf\")\n",
    "        ansi, ansj = 0, 0\n",
    "        for i,k in enumerate(embeddings):\n",
    "            #Computing Cosine distance.\n",
    "            dist =cos(k,l)\n",
    "            if(dist>minDist):\n",
    "                minDist = dist\n",
    "                ansi = i\n",
    "                ansj = j                \n",
    "        # Chosen threshold is 0.85. \n",
    "        #Threshold is determined after seeing the table in the previous cell.\n",
    "        #Name of the person identified is printed on the screen, as well as below the detecetd face (below the rectangular box).\n",
    "        if minDist > 0.85:\n",
    "            text=names[ansi]\n",
    "            # cv2.putText(im, text,(boxes[ansj][0].astype(int) ,boxes[ansj][3].astype(int) + 17), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255,255,255), 2)\n",
    "            if text==target_name: \n",
    "                return True\n",
    "\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is running on CPU, since it is already pre-trained and doesnt require GPU\n",
    "# Define Inception Resnet V1 module (GoogLe Net)\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval().to('cpu')\n",
    "transform = transforms.Compose([transforms.PILToTensor()])\n",
    "def recognize_faces(faces):\n",
    "    # generate face embeddings\n",
    "    face_embeddings = []\n",
    "    for face in faces:\n",
    "        face_embeddings.append(resnet(transform(face))) \n",
    "    return verify(face_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def background_subtraction(previous_frame, frame_resized_grayscale, min_area):\n",
    "    \"\"\"\n",
    "    This function returns 1 for the frames in which the area\n",
    "    after subtraction with previous frame is greater than minimum area\n",
    "    defined.\n",
    "    Thus expensive computation of human detection face detection\n",
    "    and face recognition is not done on all the frames.\n",
    "    Only the frames undergoing significant amount of change (which is controlled min_area)\n",
    "    are processed for detection and recognition.\n",
    "    \"\"\"\n",
    "    frameDelta = cv2.absdiff(previous_frame, frame_resized_grayscale)\n",
    "    thresh = cv2.threshold(frameDelta, 25, 255, cv2.THRESH_BINARY)[1]\n",
    "    # cv2.imshow(\"Thresh\",thresh)\n",
    "    # cv2.waitKey(200)\n",
    "    thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "    # cv2.imshow(\"Thresh dialtetd\",thresh)\n",
    "    # cv2.waitKey(200)\n",
    "    countours, _ = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for c in countours:\n",
    "        # if the contour is too small, ignore it\n",
    "        if cv2.contourArea(c) > min_area:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pic should be PIL Image. Got <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m         v_cap\u001b[38;5;241m.\u001b[39mstop()\n\u001b[1;32m     52\u001b[0m filenames \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(input_video_folder)\n\u001b[0;32m---> 53\u001b[0m \u001b[43mrun_detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfast_mtcnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [19], line 38\u001b[0m, in \u001b[0;36mrun_detection\u001b[0;34m(fast_mtcnn, filenames)\u001b[0m\n\u001b[1;32m     35\u001b[0m frames_processed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(frames)\n\u001b[1;32m     36\u001b[0m faces_detected \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(faces)\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mrecognize_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfaces\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     39\u001b[0m         timestamped_frames \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(timestamped_frames,[j,\u001b[38;5;28mtuple\u001b[39m(frames)])\n\u001b[1;32m     42\u001b[0m do_process \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn [17], line 9\u001b[0m, in \u001b[0;36mrecognize_faces\u001b[0;34m(faces)\u001b[0m\n\u001b[1;32m      7\u001b[0m face_embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m face \u001b[38;5;129;01min\u001b[39;00m faces:\n\u001b[0;32m----> 9\u001b[0m     face_embeddings\u001b[38;5;241m.\u001b[39mappend(resnet(\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface\u001b[49m\u001b[43m)\u001b[49m)) \n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m verify(face_embeddings)\n",
      "File \u001b[0;32m~/Downloads/needforspeed/env/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/Downloads/needforspeed/env/lib/python3.8/site-packages/torchvision/transforms/transforms.py:162\u001b[0m, in \u001b[0;36mPILToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m    151\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[39m    .. note::\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mpil_to_tensor(pic)\n",
      "File \u001b[0;32m~/Downloads/needforspeed/env/lib/python3.8/site-packages/torchvision/transforms/functional.py:195\u001b[0m, in \u001b[0;36mpil_to_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    193\u001b[0m     _log_api_usage_once(pil_to_tensor)\n\u001b[1;32m    194\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m F_pil\u001b[39m.\u001b[39m_is_pil_image(pic):\n\u001b[0;32m--> 195\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpic should be PIL Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(pic)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    197\u001b[0m \u001b[39mif\u001b[39;00m accimage \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(pic, accimage\u001b[39m.\u001b[39mImage):\n\u001b[1;32m    198\u001b[0m     \u001b[39m# accimage format is always uint8 internally, so always return uint8 here\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     nppic \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros([pic\u001b[39m.\u001b[39mchannels, pic\u001b[39m.\u001b[39mheight, pic\u001b[39m.\u001b[39mwidth], dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39muint8)\n",
      "\u001b[0;31mTypeError\u001b[0m: pic should be PIL Image. Got <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "def run_detection(fast_mtcnn, filenames):\n",
    "    frames = []\n",
    "    frames_processed = 0\n",
    "    faces_detected = 0\n",
    "    batch_size = 15\n",
    "    start = time.time()\n",
    "    do_process = []\n",
    "    \n",
    "\n",
    "    for filename in filenames:\n",
    "\n",
    "        v_cap = FileVideoStream(filename).start()\n",
    "        v_len = int(v_cap.stream.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        assert v_len>0, \"Corrupt video found\"\n",
    "        prev_frame = v_cap.read()\n",
    "        prev_frame_grey = cv2.cvtColor(prev_frame,cv2.COLOR_BGR2GRAY)\n",
    "        # min area for background subtraction\n",
    "        min_area = (3000 / 1280) * prev_frame.shape[1]\n",
    "\n",
    "\n",
    "        for j in range(v_len):\n",
    "            frame = v_cap.read()\n",
    "            # background subtraction\n",
    "            frame_grey = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "            do_process.append(background_subtraction(prev_frame_grey,frame_grey,min_area))\n",
    "            prev_frame_grey = frame_grey\n",
    "\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) >= batch_size or j == v_len - 1:\n",
    "\n",
    "                faces = fast_mtcnn(frames,do_process)\n",
    "\n",
    "                frames_processed += len(frames)\n",
    "                faces_detected += len(faces)\n",
    "                \n",
    "                if recognize_faces(faces):\n",
    "                        timestamped_frames = np.append(timestamped_frames,[j,tuple(frames)])\n",
    "\n",
    "\n",
    "                do_process = []\n",
    "                frames = []\n",
    "                print(\n",
    "                    f'Frames per second: {frames_processed / (time.time() - start):.3f},',\n",
    "                    f'faces detected: {faces_detected}\\r',\n",
    "                    end=''\n",
    "                )\n",
    "\n",
    "        v_cap.stop()\n",
    "\n",
    "filenames = glob.glob(input_video_folder)\n",
    "run_detection(fast_mtcnn, filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frame Stitching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0], dtype=object)]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "No person detected",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m timestamped_frames \u001b[38;5;241m=\u001b[39m timestamped_frames[timestamped_frames[:,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margsort()]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m([timestamped_frames[:,\u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(timestamped_frames)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo person detected\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m height, width, layers \u001b[38;5;241m=\u001b[39m timestamped_frames[\u001b[38;5;241m3\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Decaaring video writer\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: No person detected"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# np.random.shuffle(timestamped_frames)\n",
    "timestamped_frames = timestamped_frames[timestamped_frames[:,0].argsort()]\n",
    "print([timestamped_frames[:,0]])\n",
    "assert len(timestamped_frames)>1, \"No person detected\"\n",
    "height, width, layers = timestamped_frames[3][-1][-1].shape\n",
    "# Decaaring video writer\n",
    "writer = cv2.VideoWriter(output_video_name,cv2.VideoWriter_fourcc(*'MPEG'),output_fps,(width,height))\n",
    "assert writer, \"Error in creating video writer\"\n",
    "            \n",
    "for i,name,frames in timestamped_frames[1:]:\n",
    "    for frame in frames:\n",
    "        if frame is not None: writer.write(frame)\n",
    "writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Half resolution detection\n",
    "\n",
    "In this example, we demonstrate how to detect faces using half resolution frames (i.e., `resize=0.5`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_mtcnn = FastMTCNN(\n",
    "    stride=4,\n",
    "    resize=0.5,\n",
    "    margin=14,\n",
    "    factor=0.5,\n",
    "    keep_all=True,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second: 42.454, faces detected: 2178\r"
     ]
    }
   ],
   "source": [
    "run_detection(fast_mtcnn, filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "54d8a45ce5b0410bd851d9c3ac2ace8c44e2082abce744e94687b86b878ce3ed"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1097ab2cbee749658365fd576dd76510": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "36e8c4801c7b4b0cb33deaa8e1e4edfe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b92158aaca674063abb4a57107f2238c",
       "placeholder": "​",
       "style": "IPY_MODEL_d5a75be943dd4c06b738005bd22926ad",
       "value": " 100/100 [11:27&lt;00:00,  6.87s/it]"
      }
     },
     "38825bb622cf4ef78c9210b2a0a27abc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_950d8226a3d3497fbcda71810be071b3",
        "IPY_MODEL_36e8c4801c7b4b0cb33deaa8e1e4edfe"
       ],
       "layout": "IPY_MODEL_a11e91858bdb42ebbd50979012b86be5"
      }
     },
     "485982d49f51484f9c7618f7bcd807f6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4a7a68a20376498c8407da2038cff6c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bdaaf2b207bc4a29a47e3ddb626cb7b3",
       "placeholder": "​",
       "style": "IPY_MODEL_485982d49f51484f9c7618f7bcd807f6",
       "value": " 100/100 [07:35&lt;00:00,  4.56s/it]"
      }
     },
     "6571baa0462c4d9bb821d9b28cc3321d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fc69a3fd6e4b41a08faca97e92bfc644",
        "IPY_MODEL_4a7a68a20376498c8407da2038cff6c6"
       ],
       "layout": "IPY_MODEL_6a93b9610ed64ef88e7b6eb60f87eca7"
      }
     },
     "6a93b9610ed64ef88e7b6eb60f87eca7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "950d8226a3d3497fbcda71810be071b3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d933839ef9394df9969e9410f5b8dcc9",
       "max": 100,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1097ab2cbee749658365fd576dd76510",
       "value": 100
      }
     },
     "a11e91858bdb42ebbd50979012b86be5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b0ab429894084814baf5fbc8d7b6107d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b92158aaca674063abb4a57107f2238c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bdaaf2b207bc4a29a47e3ddb626cb7b3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d5a75be943dd4c06b738005bd22926ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d933839ef9394df9969e9410f5b8dcc9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f3b2b95db2464a828e9cfd09341b538d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "fc69a3fd6e4b41a08faca97e92bfc644": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b0ab429894084814baf5fbc8d7b6107d",
       "max": 100,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f3b2b95db2464a828e9cfd09341b538d",
       "value": 100
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
