{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast MTCNN detector\n",
    "\n",
    "This notebook demonstrates how to achieve 45 frames per second speeds for loading frames and detecting faces on full resolution videos.\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "**Striding**: The algorithm used is a strided modification of MTCNN in which face detection is performed on only every _N_ frames, and applied to all frames. For example, with a batch of 9 frames, we could pass frames 0, 3, and 6 to MTCNN. Then, the bounding boxes (and potentially landmarks) returned for frame 0 would be naively applied to frames 1 and 2. Similarly, the detections for frame 3 are applied to frames 4 and 5, and the detections for frames 6 are applied to frames 7 and 8.\n",
    "\n",
    "Although this assume that faces do not move between frames significantly, this is generally a good approximation for low stride numbers. If the stride is 3, we are assuming that the face does not significantly alter position for an additional 2 frames, or ~0.07 seconds. If faces are moving faster than this, they are likely to be extremely blurry anyway. Furthermore, ensuring that faces are cropped with a small margin mitigates the impact of face drift.\n",
    "\n",
    "**Scale pyramid**: The algorithm uses a slightly smaller scaling factor (0.6 vs 0.709) than the original MTCNN algorithm to construct the scaling pyramid applied to input images. For details of the scaling pyramid, see the [original paper](https://arxiv.org/abs/1604.02878) for details of the scaling pyramid approach.\n",
    "\n",
    "**Multi-threading**: A modest performance gain comes from loading video frames (with `cv2.VideoCapture`) using threading. This functionality is provided by the `FileVideoStream` class of the imutils package.\n",
    "\n",
    "## Other resources\n",
    "\n",
    "See the following kernel for a guide to using the MTCNN functionality of facenet-pytorch: https://www.kaggle.com/timesler/guide-to-mtcnn-in-facenet-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from PIL import Image\n",
    "import torch\n",
    "from imutils.video import FileVideoStream\n",
    "import cv2\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets,transforms\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(torch.cuda.is_available())\n",
    "input_video_folder = './videos/*.mp4'\n",
    "images_folder = './frames/'\n",
    "target_name = 'divyansh'\n",
    "filenames = glob.glob(input_video_folder)\n",
    "confidence = 0.85\n",
    "stride = 5\n",
    "batch_size = 25\n",
    "output_fps = 23\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Face Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face detected with probability: 0.999979\n",
      "Face detected with probability: 0.999989\n",
      "Face detected with probability: 0.999998\n",
      "Face detected with probability: 0.999979\n",
      "Face detected with probability: 1.000000\n",
      "Face detected with probability: 0.999993\n",
      "Face detected with probability: 0.999815\n",
      "Face detected with probability: 0.999996\n",
      "Face detected with probability: 1.000000\n",
      "Face detected with probability: 1.000000\n",
      "Face detected with probability: 0.999995\n",
      "Face detected with probability: 0.999925\n",
      "Face detected with probability: 1.000000\n",
      "Face detected with probability: 0.999997\n",
      "Face detected with probability: 0.999987\n",
      "Face detected with probability: 0.999992\n",
      "Face detected with probability: 0.999974\n",
      "Face detected with probability: 0.999664\n",
      "Face detected with probability: 0.999714\n",
      "Face detected with probability: 0.999993\n",
      "Face detected with probability: 0.999993\n",
      "Face detected with probability: 0.999991\n",
      "Face detected with probability: 0.999990\n",
      "Face detected with probability: 0.999997\n",
      "Face detected with probability: 0.999999\n",
      "Face detected with probability: 0.999994\n",
      "Face detected with probability: 1.000000\n",
      "Face detected with probability: 0.999999\n",
      "Face detected with probability: 0.999970\n",
      "Face detected with probability: 0.999997\n",
      "Face detected with probability: 0.999997\n",
      "Face detected with probability: 0.999994\n",
      "Face detected with probability: 0.999992\n",
      "Face detected with probability: 0.999968\n",
      "Face detected with probability: 0.999958\n",
      "Face detected with probability: 0.999973\n",
      "Face detected with probability: 0.999881\n",
      "Face detected with probability: 0.999810\n",
      "Face detected with probability: 0.999861\n",
      "          divyansh  divyansh  divyansh  divyansh  divyansh  divyansh  \\\n",
      "divyansh  1.000000  0.835695  0.887559  0.934867  0.900665  0.887497   \n",
      "divyansh  0.835695  1.000000  0.901821  0.836760  0.869600  0.850042   \n",
      "divyansh  0.887559  0.901821  1.000000  0.872119  0.936130  0.879097   \n",
      "divyansh  0.934867  0.836760  0.872119  1.000000  0.882221  0.876261   \n",
      "divyansh  0.900665  0.869600  0.936130  0.882221  1.000000  0.957632   \n",
      "divyansh  0.887497  0.850042  0.879097  0.876261  0.957632  1.000000   \n",
      "divyansh  0.935725  0.842149  0.846003  0.927172  0.877491  0.877031   \n",
      "divyansh  0.801244  0.845795  0.874945  0.836833  0.862735  0.869136   \n",
      "divyansh  0.853979  0.794638  0.867443  0.875612  0.874335  0.868860   \n",
      "divyansh  0.849813  0.848390  0.885652  0.869346  0.901287  0.914551   \n",
      "divyansh  0.832995  0.826676  0.852855  0.830886  0.881984  0.907371   \n",
      "divyansh  0.792203  0.786784  0.813186  0.798594  0.833097  0.878052   \n",
      "divyansh  0.804032  0.769959  0.804014  0.792685  0.840620  0.876245   \n",
      "divyansh  0.795681  0.790838  0.800962  0.778414  0.760522  0.767917   \n",
      "divyansh  0.756910  0.799224  0.785157  0.762624  0.742697  0.745609   \n",
      "divyansh  0.786427  0.793788  0.803195  0.796353  0.759530  0.765547   \n",
      "divyansh  0.795730  0.808390  0.821229  0.801033  0.778121  0.770623   \n",
      "divyansh  0.740915  0.823072  0.742837  0.778063  0.696009  0.723185   \n",
      "divyansh  0.771937  0.875768  0.821200  0.779966  0.808385  0.811582   \n",
      "divyansh  0.793460  0.868931  0.838182  0.802508  0.826537  0.823176   \n",
      "divyansh  0.785247  0.848571  0.850228  0.810925  0.824236  0.814521   \n",
      "divyansh  0.849823  0.879214  0.905510  0.856735  0.846706  0.810713   \n",
      "divyansh  0.801732  0.861083  0.888292  0.804510  0.827266  0.792461   \n",
      "divyansh  0.840757  0.883428  0.905164  0.821095  0.843674  0.805343   \n",
      "divyansh  0.844295  0.865744  0.915836  0.831518  0.875718  0.845272   \n",
      "divyansh  0.848554  0.869399  0.910545  0.840100  0.890608  0.878407   \n",
      "divyansh  0.864250  0.863198  0.914768  0.861257  0.889423  0.876252   \n",
      "divyansh  0.847702  0.863145  0.897305  0.843063  0.897945  0.901944   \n",
      "divyansh  0.858280  0.865368  0.873349  0.828379  0.906213  0.909916   \n",
      "divyansh  0.877777  0.866420  0.892254  0.850237  0.895797  0.897440   \n",
      "divyansh  0.882788  0.854887  0.884245  0.864701  0.874240  0.885155   \n",
      "divyansh  0.868850  0.877055  0.906151  0.851220  0.829479  0.814207   \n",
      "divyansh  0.860021  0.883725  0.880403  0.826460  0.824802  0.825466   \n",
      "divyansh  0.894969  0.871541  0.886388  0.859121  0.849588  0.854524   \n",
      "divyansh  0.880239  0.916452  0.904466  0.862964  0.863966  0.871229   \n",
      "divyansh  0.876469  0.883954  0.851219  0.832513  0.837603  0.831224   \n",
      "divyansh  0.810764  0.860335  0.829752  0.823245  0.782848  0.782534   \n",
      "divyansh  0.690009  0.697055  0.726147  0.698487  0.682118  0.688273   \n",
      "divyansh  0.808864  0.895249  0.885296  0.807304  0.817952  0.786126   \n",
      "\n",
      "          divyansh  divyansh  divyansh  divyansh  ...  divyansh  divyansh  \\\n",
      "divyansh  0.935725  0.801244  0.853979  0.849813  ...  0.877777  0.882788   \n",
      "divyansh  0.842149  0.845795  0.794638  0.848390  ...  0.866420  0.854887   \n",
      "divyansh  0.846003  0.874945  0.867443  0.885652  ...  0.892254  0.884245   \n",
      "divyansh  0.927172  0.836833  0.875612  0.869346  ...  0.850237  0.864701   \n",
      "divyansh  0.877491  0.862735  0.874335  0.901287  ...  0.895797  0.874240   \n",
      "divyansh  0.877031  0.869136  0.868860  0.914551  ...  0.897440  0.885155   \n",
      "divyansh  1.000000  0.781554  0.811939  0.835268  ...  0.841292  0.847787   \n",
      "divyansh  0.781554  1.000000  0.948242  0.950552  ...  0.904491  0.898267   \n",
      "divyansh  0.811939  0.948242  1.000000  0.954834  ...  0.870500  0.870723   \n",
      "divyansh  0.835268  0.950552  0.954834  1.000000  ...  0.887073  0.873898   \n",
      "divyansh  0.789833  0.928413  0.926222  0.971586  ...  0.906397  0.882930   \n",
      "divyansh  0.761194  0.905075  0.895524  0.940261  ...  0.889746  0.873649   \n",
      "divyansh  0.765672  0.904362  0.900048  0.928382  ...  0.903119  0.886751   \n",
      "divyansh  0.761582  0.854976  0.851347  0.848314  ...  0.874641  0.899441   \n",
      "divyansh  0.740238  0.849789  0.828153  0.842330  ...  0.858121  0.885937   \n",
      "divyansh  0.769309  0.852258  0.833323  0.848883  ...  0.865772  0.895780   \n",
      "divyansh  0.787931  0.785154  0.777659  0.796843  ...  0.825479  0.853623   \n",
      "divyansh  0.746689  0.775675  0.745526  0.769811  ...  0.806177  0.831474   \n",
      "divyansh  0.795943  0.838755  0.804821  0.833794  ...  0.846567  0.845626   \n",
      "divyansh  0.807680  0.876349  0.820294  0.833856  ...  0.893189  0.896578   \n",
      "divyansh  0.776143  0.954524  0.881415  0.870265  ...  0.894495  0.894767   \n",
      "divyansh  0.801889  0.882673  0.838132  0.841185  ...  0.881508  0.871146   \n",
      "divyansh  0.774627  0.881848  0.816695  0.824683  ...  0.870747  0.861425   \n",
      "divyansh  0.796892  0.883253  0.830273  0.839120  ...  0.890652  0.881030   \n",
      "divyansh  0.800941  0.908903  0.874681  0.867740  ...  0.929784  0.899742   \n",
      "divyansh  0.817085  0.931065  0.884676  0.888085  ...  0.951760  0.922281   \n",
      "divyansh  0.831635  0.925818  0.891520  0.881804  ...  0.955071  0.931209   \n",
      "divyansh  0.816643  0.911278  0.870544  0.881713  ...  0.971757  0.941953   \n",
      "divyansh  0.823794  0.882837  0.853014  0.872359  ...  0.972116  0.946377   \n",
      "divyansh  0.841292  0.904491  0.870500  0.887073  ...  1.000000  0.976472   \n",
      "divyansh  0.847787  0.898267  0.870723  0.873898  ...  0.976472  1.000000   \n",
      "divyansh  0.823332  0.868403  0.853299  0.842178  ...  0.903889  0.921513   \n",
      "divyansh  0.823999  0.837549  0.821979  0.821722  ...  0.911072  0.926508   \n",
      "divyansh  0.867017  0.851823  0.840435  0.846401  ...  0.923142  0.944281   \n",
      "divyansh  0.848023  0.898974  0.858565  0.876613  ...  0.945130  0.947791   \n",
      "divyansh  0.876777  0.824798  0.815175  0.832348  ...  0.899450  0.897458   \n",
      "divyansh  0.807098  0.793531  0.763086  0.784775  ...  0.826010  0.803698   \n",
      "divyansh  0.674901  0.729634  0.688127  0.697342  ...  0.729536  0.726624   \n",
      "divyansh  0.778309  0.868052  0.798494  0.825800  ...  0.891854  0.870815   \n",
      "\n",
      "          divyansh  divyansh  divyansh  divyansh  divyansh  divyansh  \\\n",
      "divyansh  0.868850  0.860021  0.894969  0.880239  0.876469  0.810764   \n",
      "divyansh  0.877055  0.883725  0.871541  0.916452  0.883954  0.860335   \n",
      "divyansh  0.906151  0.880403  0.886388  0.904466  0.851219  0.829752   \n",
      "divyansh  0.851220  0.826460  0.859121  0.862964  0.832513  0.823245   \n",
      "divyansh  0.829479  0.824802  0.849588  0.863966  0.837603  0.782848   \n",
      "divyansh  0.814207  0.825466  0.854524  0.871229  0.831224  0.782534   \n",
      "divyansh  0.823332  0.823999  0.867017  0.848023  0.876777  0.807098   \n",
      "divyansh  0.868403  0.837549  0.851823  0.898974  0.824798  0.793531   \n",
      "divyansh  0.853299  0.821979  0.840435  0.858565  0.815175  0.763086   \n",
      "divyansh  0.842178  0.821722  0.846401  0.876613  0.832348  0.784775   \n",
      "divyansh  0.816747  0.808995  0.831647  0.869420  0.821466  0.764148   \n",
      "divyansh  0.772861  0.767922  0.791043  0.838103  0.795423  0.719224   \n",
      "divyansh  0.783815  0.786081  0.809610  0.846774  0.795378  0.717247   \n",
      "divyansh  0.862504  0.857134  0.872131  0.878658  0.807579  0.747835   \n",
      "divyansh  0.859160  0.855703  0.863359  0.872512  0.800964  0.740798   \n",
      "divyansh  0.859981  0.852967  0.870281  0.879527  0.813562  0.766894   \n",
      "divyansh  0.868837  0.865995  0.879629  0.874112  0.825493  0.786009   \n",
      "divyansh  0.821075  0.835702  0.845475  0.862290  0.802685  0.799567   \n",
      "divyansh  0.802314  0.830587  0.841158  0.859203  0.815372  0.872644   \n",
      "divyansh  0.838676  0.862125  0.865187  0.896806  0.819157  0.865501   \n",
      "divyansh  0.862690  0.839898  0.849301  0.888653  0.809500  0.819587   \n",
      "divyansh  0.903105  0.886666  0.893862  0.911562  0.829472  0.863795   \n",
      "divyansh  0.859801  0.842565  0.866337  0.895510  0.793162  0.849106   \n",
      "divyansh  0.887125  0.873046  0.889720  0.907186  0.830413  0.857859   \n",
      "divyansh  0.884437  0.872191  0.885643  0.904521  0.825775  0.837703   \n",
      "divyansh  0.876726  0.867282  0.891504  0.917566  0.852672  0.827144   \n",
      "divyansh  0.898549  0.886658  0.899674  0.924344  0.858290  0.831293   \n",
      "divyansh  0.871193  0.865721  0.874242  0.912913  0.871119  0.840157   \n",
      "divyansh  0.865124  0.879880  0.885141  0.911937  0.886240  0.805647   \n",
      "divyansh  0.903889  0.911072  0.923142  0.945130  0.899450  0.826010   \n",
      "divyansh  0.921513  0.926508  0.944281  0.947791  0.897458  0.803698   \n",
      "divyansh  1.000000  0.978256  0.962046  0.950530  0.901511  0.821243   \n",
      "divyansh  0.978256  1.000000  0.970507  0.954157  0.905228  0.812422   \n",
      "divyansh  0.962046  0.970507  1.000000  0.967313  0.927413  0.830643   \n",
      "divyansh  0.950530  0.954157  0.967313  1.000000  0.926067  0.853606   \n",
      "divyansh  0.901511  0.905228  0.927413  0.926067  1.000000  0.836441   \n",
      "divyansh  0.821243  0.812422  0.830643  0.853606  0.836441  1.000000   \n",
      "divyansh  0.714108  0.700673  0.735426  0.733008  0.711808  0.915552   \n",
      "divyansh  0.885202  0.880364  0.876422  0.901281  0.826763  0.906486   \n",
      "\n",
      "          divyansh  divyansh  \n",
      "divyansh  0.690009  0.808864  \n",
      "divyansh  0.697055  0.895249  \n",
      "divyansh  0.726147  0.885296  \n",
      "divyansh  0.698487  0.807304  \n",
      "divyansh  0.682118  0.817952  \n",
      "divyansh  0.688273  0.786126  \n",
      "divyansh  0.674901  0.778309  \n",
      "divyansh  0.729634  0.868052  \n",
      "divyansh  0.688127  0.798494  \n",
      "divyansh  0.697342  0.825800  \n",
      "divyansh  0.696038  0.806596  \n",
      "divyansh  0.667148  0.768716  \n",
      "divyansh  0.667260  0.770546  \n",
      "divyansh  0.711835  0.793960  \n",
      "divyansh  0.703279  0.798521  \n",
      "divyansh  0.728283  0.790608  \n",
      "divyansh  0.718287  0.763295  \n",
      "divyansh  0.715311  0.767880  \n",
      "divyansh  0.817265  0.884569  \n",
      "divyansh  0.796383  0.907890  \n",
      "divyansh  0.768371  0.903599  \n",
      "divyansh  0.797500  0.959854  \n",
      "divyansh  0.805621  0.956586  \n",
      "divyansh  0.797366  0.959044  \n",
      "divyansh  0.772576  0.936034  \n",
      "divyansh  0.755756  0.911510  \n",
      "divyansh  0.749576  0.907149  \n",
      "divyansh  0.761295  0.885437  \n",
      "divyansh  0.715287  0.854395  \n",
      "divyansh  0.729536  0.891854  \n",
      "divyansh  0.726624  0.870815  \n",
      "divyansh  0.714108  0.885202  \n",
      "divyansh  0.700673  0.880364  \n",
      "divyansh  0.735426  0.876422  \n",
      "divyansh  0.733008  0.901281  \n",
      "divyansh  0.711808  0.826763  \n",
      "divyansh  0.915552  0.906486  \n",
      "divyansh  1.000000  0.828476  \n",
      "divyansh  0.828476  1.000000  \n",
      "\n",
      "[39 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "# # The model is running on CPU, since it is already pre-trained and doesnt require GPU\n",
    "# device = 'cpu'\n",
    "# print('Running on device: {}'.format(device))\n",
    "\n",
    "#Define MTCNN module\n",
    "#Since MTCNN is a collection of neural nets and other code, \n",
    "#The device must be passed in the following way to enable copying of objects when needed internally.\n",
    "mtcnn = MTCNN(\n",
    "    image_size=160, margin=0, min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7], factor=0.709,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "#Function takes 2 vectors 'a' and 'b'\n",
    "#Returns the cosine similarity according to the definition of the dot product\n",
    "def cos_sim(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "#cos_sim returns real numbers,where negative numbers have different interpretations.\n",
    "#So we use this function to return only positive values.\n",
    "def cos(a,b):\n",
    "    minx = -1 \n",
    "    maxx = 1\n",
    "    return (cos_sim(a,b)- minx)/(maxx-minx)\n",
    "\n",
    "# Define Inception Resnet V1 module (GoogLe Net)\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "# Define a dataset and data loader\n",
    "dataset = datasets.ImageFolder(images_folder)\n",
    "dataset.idx_to_class = {i:c for c, i in dataset.class_to_idx.items()}\n",
    "loader = DataLoader(dataset, collate_fn=lambda x: x[0])\n",
    "\n",
    "#Perfom MTCNN facial detection\n",
    "#Detects the face present in the image and prints the probablity of face detected in the image.\n",
    "aligned = []\n",
    "names = []\n",
    "for x, y in loader:\n",
    "    x_aligned, prob = mtcnn(x, return_prob=True)\n",
    "    if x_aligned is not None:\n",
    "        print('Face detected with probability: {:8f}'.format(prob))\n",
    "        aligned.append(x_aligned)\n",
    "        names.append(dataset.idx_to_class[y])\n",
    "\n",
    "# Calculate the 512 face embeddings\n",
    "aligned = torch.stack(aligned).to(device)\n",
    "embeddings = resnet(aligned).cpu()\n",
    "\n",
    "# Print distance matrix for classes.\n",
    "#The embeddings are plotted in space and cosine distace is measured.\n",
    "cos_sim = nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "for i in range(0,len(names)):\n",
    "    emb=embeddings[i].unsqueeze(0)\n",
    "    # The cosine similarity between the embeddings is given by 'dist'.\n",
    "    dist =cos(embeddings[0],emb)  \n",
    "        \n",
    "dists = [[cos(e1,e2).item() for e2 in embeddings] for e1 in embeddings]\n",
    "# The print statement below is\n",
    "#Helpful for analysing the results and for determining the value of threshold.\n",
    "print(pd.DataFrame(dists, columns=names, index=names)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The FastMTCNN class\n",
    "\n",
    "The class below is a thin wrapper for the MTCNN implementation in the `facenet-pytorch` package that implements the algorithm described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastMTCNN(object):\n",
    "    \"\"\"Fast MTCNN implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, stride, resize=1, *args, **kwargs):\n",
    "        \"\"\"Constructor for FastMTCNN class.\n",
    "        \n",
    "        Arguments:\n",
    "            stride (int): The detection stride. Faces will be detected every `stride` frames\n",
    "                and remembered for `stride-1` frames.\n",
    "        \n",
    "        Keyword arguments:\n",
    "            resize (float): Fractional frame scaling. [default: {1}]\n",
    "            *args: Arguments to pass to the MTCNN constructor. See help(MTCNN).\n",
    "            **kwargs: Keyword arguments to pass to the MTCNN constructor. See help(MTCNN).\n",
    "        \"\"\"\n",
    "        self.stride = stride\n",
    "        self.resize = resize\n",
    "        self.mtcnn = MTCNN(*args, **kwargs)\n",
    "        \n",
    "    def __call__(self, frames):\n",
    "        \"\"\"Detect faces in frames using strided MTCNN.\"\"\"\n",
    "        if self.resize != 1:\n",
    "            frames = [\n",
    "                cv2.resize(f, (int(f.shape[1] * self.resize), int(f.shape[0] * self.resize)))\n",
    "                    for f in frames\n",
    "            ]\n",
    "                      \n",
    "        # boxes, probs = self.mtcnn.detect(frames[::self.stride])\n",
    "        \n",
    "        faces = []\n",
    "        batched_frames = []\n",
    "\n",
    "        for i, frame in enumerate(frames):\n",
    "            if(i%self.stride == 0):\n",
    "                batched_frames.append(frame)\n",
    "\n",
    "        faces = self.mtcnn(batched_frames)\n",
    "            # box_ind = int(i / self.stride)\n",
    "            # if boxes[box_ind] is None:\n",
    "            #     continue\n",
    "            # for box in boxes[box_ind]:\n",
    "            #     box = [int(b) for b in box]\n",
    "            #     faces.append(frame[box[1]:box[3], box[0]:box[2]])\n",
    "            \n",
    "        return faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full resolution detection\n",
    "\n",
    "In this example, we demonstrate how to detect faces using full resolution frames (i.e., `resize=1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_mtcnn = FastMTCNN(\n",
    "    image_size=160, min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7],\n",
    "    stride=stride,\n",
    "    resize=1,\n",
    "    margin=14,\n",
    "    factor=0.6,\n",
    "    keep_all=True,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognize Faces   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(faces): \n",
    "    for j,l in enumerate(faces):\n",
    "        minDist = float(\"-inf\")\n",
    "        ansi, ansj = 0, 0\n",
    "        for i,k in enumerate(embeddings):\n",
    "            #Computing Cosine distance.\n",
    "            dist =cos(k,l)\n",
    "            if(dist>minDist):\n",
    "                minDist = dist\n",
    "                ansi = i\n",
    "                ansj = j                \n",
    "        # Chosen threshold is 0.85. \n",
    "        #Threshold is determined after seeing the table in the previous cell.\n",
    "        #Name of the person identified is printed on the screen, as well as below the detecetd face (below the rectangular box).\n",
    "        if minDist > confidence:\n",
    "            text=names[ansi]\n",
    "            # cv2.putText(im, text,(boxes[ansj][0].astype(int) ,boxes[ansj][3].astype(int) + 17), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255,255,255), 2)\n",
    "            if text==target_name: \n",
    "                return True\n",
    "\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is running on CPU, since it is already pre-trained and doesnt require GPU\n",
    "# Define Inception Resnet V1 module (GoogLe Net)\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval().to('cpu')\n",
    "def recognize_faces(batch):\n",
    "    # generate face embeddings\n",
    "    isPresent = 0\n",
    "    face_embeddings = []\n",
    "    for facesInImage in batch:\n",
    "        if facesInImage is None or not len(facesInImage[0])>0: continue\n",
    "        embeddingsInImage = resnet(facesInImage)\n",
    "        if(verify(embeddingsInImage)):\n",
    "            isPresent = 1\n",
    "            break\n",
    "\n",
    "    return isPresent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def background_subtraction(previous_frame, frame_resized_grayscale, min_area):\n",
    "    \"\"\"\n",
    "    This function returns 1 for the frames in which the area\n",
    "    after subtraction with previous frame is greater than minimum area\n",
    "    defined.\n",
    "    Thus expensive computation of human detection face detection\n",
    "    and face recognition is not done on all the frames.\n",
    "    Only the frames undergoing significant amount of change (which is controlled min_area)\n",
    "    are processed for detection and recognition.\n",
    "    \"\"\"\n",
    "    frameDelta = cv2.absdiff(previous_frame, frame_resized_grayscale)\n",
    "    thresh = cv2.threshold(frameDelta, 25, 255, cv2.THRESH_BINARY)[1]\n",
    "    # cv2.imshow(\"Thresh\",thresh)\n",
    "    # cv2.waitKey(200)\n",
    "    thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "    # cv2.imshow(\"Thresh dialtetd\",thresh)\n",
    "    # cv2.waitKey(200)\n",
    "    countours, _ = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for c in countours:\n",
    "        # if the contour is too small, ignore it\n",
    "        if cv2.contourArea(c) > min_area:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3919/2164699459.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  timestamped_frames = np.array([[0,[None]]])\n",
      "/home/group1/Downloads/needforspeed/env/lib/python3.8/site-packages/facenet_pytorch/models/utils/detect_face.py:183: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  batch_boxes, batch_points = np.array(batch_boxes), np.array(batch_points)\n",
      "/home/group1/Downloads/needforspeed/env/lib/python3.8/site-packages/facenet_pytorch/models/mtcnn.py:339: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  boxes = np.array(boxes)\n",
      "/home/group1/Downloads/needforspeed/env/lib/python3.8/site-packages/facenet_pytorch/models/mtcnn.py:340: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  probs = np.array(probs)\n",
      "/home/group1/Downloads/needforspeed/env/lib/python3.8/site-packages/facenet_pytorch/models/mtcnn.py:341: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  points = np.array(points)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second: 28.404, faces detected: 95\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second: 30.215, faces detected: 1180\r"
     ]
    }
   ],
   "source": [
    "def run_detection(fast_mtcnn, filenames):\n",
    "    frames = []\n",
    "    global frames_processed\n",
    "    global faces_detected\n",
    "    global start\n",
    "    frames_processed = 0\n",
    "    faces_detected = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    global timestamped_frames\n",
    "    # dtype = [('name', int), ('frame', list)]\n",
    "    timestamped_frames = np.array([[0,[None]]])\n",
    "    for filename in filenames:\n",
    "\n",
    "        v_cap = FileVideoStream(filename).start()\n",
    "        v_len = int(v_cap.stream.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        assert v_len>0, \"Corrupt video found\"\n",
    "        prev_frame = v_cap.read()\n",
    "        prev_frame_grey = cv2.cvtColor(prev_frame,cv2.COLOR_BGR2GRAY)\n",
    "        # min area for background subtraction\n",
    "        min_area = (3000 / 1280) * prev_frame.shape[1]\n",
    "        \n",
    "\n",
    "        # nparray = np.array([0,tuple()],dtype=object)\n",
    "        for j in range(v_len):\n",
    "            frame = v_cap.read()\n",
    "            # background subtraction\n",
    "            if frame is None: continue\n",
    "            frame_grey = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "            if not background_subtraction(prev_frame_grey,frame_grey,min_area):\n",
    "                continue\n",
    "            prev_frame_grey = frame_grey\n",
    "\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) >= batch_size or j == v_len - 1:\n",
    "\n",
    "                faces = fast_mtcnn(frames)\n",
    "\n",
    "                frames_processed += len(frames)\n",
    "                faces_detected += len(faces)\n",
    "                \n",
    "\n",
    "                # for face in faces:\n",
    "                #     print(face)\n",
    "                \n",
    "                if recognize_faces(faces):\n",
    "                        timestamped_frames = np.append(timestamped_frames,[[j,frames if frames else [None]]],axis=0)\n",
    "\n",
    "\n",
    "                frames = []\n",
    "                print(\n",
    "                    f'Frames per second: {frames_processed / (time.time() - start):.3f},',\n",
    "                    f'faces detected: {faces_detected}\\r',\n",
    "                    end=''\n",
    "                )\n",
    "\n",
    "        # timestamped_frames = np.append(timestamped_frames, nparray,axis=0)\n",
    "\n",
    "        v_cap.stop()\n",
    "\n",
    "filenames = glob.glob(input_video_folder)\n",
    "process_time = time.time()\n",
    "run_detection(fast_mtcnn, filenames)\n",
    "after_process_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frame Stitching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 501, 502, 526, 527, 551, 552, 576, 577, 601, 602, 626, 627, 651,\n",
      "       652, 676, 679, 1055, 1130, 1155, 1205, 1230, 1255, 1280, 1305,\n",
      "       1330, 1355, 1380, 1534, 1536, 1559, 1561, 1584, 1586, 1609, 1611],\n",
      "      dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# np.random.shuffle(timestamped_frames)\n",
    "# print(timestamped_frames[:,0])\n",
    "output_video_name = f\"output-{target_name}-{filenames[0].split('/')[-1]}-{len(filenames)}-pt{int(after_process_time-process_time)}-c{confidence*100}-batch{batch_size}-s{stride}.avi\"\n",
    "timestamped_frames = timestamped_frames[timestamped_frames[:,0].argsort()]\n",
    "print([timestamped_frames[:,0]])\n",
    "assert len(timestamped_frames)>1, \"No person detected\"\n",
    "height, width, layers = timestamped_frames[3][-1][-1].shape\n",
    "# Decaaring video writer\n",
    "writer = cv2.VideoWriter(output_video_name,cv2.VideoWriter_fourcc(*'MPEG'),output_fps,(width,height))\n",
    "assert writer, \"Error in creating video writer\"\n",
    "            \n",
    "for i,frames in timestamped_frames[1:]:\n",
    "    for frame in frames:\n",
    "        if frame is not None: \n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            writer.write(frame)\n",
    "writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Half resolution detection\n",
    "\n",
    "In this example, we demonstrate how to detect faces using half resolution frames (i.e., `resize=0.5`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast_mtcnn = FastMTCNN(\n",
    "#     stride=4,\n",
    "#     resize=0.5,\n",
    "#     margin=14,\n",
    "#     factor=0.5,\n",
    "#     keep_all=True,\n",
    "#     device=device\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_detection(fast_mtcnn, filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "54d8a45ce5b0410bd851d9c3ac2ace8c44e2082abce744e94687b86b878ce3ed"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1097ab2cbee749658365fd576dd76510": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "36e8c4801c7b4b0cb33deaa8e1e4edfe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b92158aaca674063abb4a57107f2238c",
       "placeholder": "​",
       "style": "IPY_MODEL_d5a75be943dd4c06b738005bd22926ad",
       "value": " 100/100 [11:27&lt;00:00,  6.87s/it]"
      }
     },
     "38825bb622cf4ef78c9210b2a0a27abc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_950d8226a3d3497fbcda71810be071b3",
        "IPY_MODEL_36e8c4801c7b4b0cb33deaa8e1e4edfe"
       ],
       "layout": "IPY_MODEL_a11e91858bdb42ebbd50979012b86be5"
      }
     },
     "485982d49f51484f9c7618f7bcd807f6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4a7a68a20376498c8407da2038cff6c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bdaaf2b207bc4a29a47e3ddb626cb7b3",
       "placeholder": "​",
       "style": "IPY_MODEL_485982d49f51484f9c7618f7bcd807f6",
       "value": " 100/100 [07:35&lt;00:00,  4.56s/it]"
      }
     },
     "6571baa0462c4d9bb821d9b28cc3321d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fc69a3fd6e4b41a08faca97e92bfc644",
        "IPY_MODEL_4a7a68a20376498c8407da2038cff6c6"
       ],
       "layout": "IPY_MODEL_6a93b9610ed64ef88e7b6eb60f87eca7"
      }
     },
     "6a93b9610ed64ef88e7b6eb60f87eca7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "950d8226a3d3497fbcda71810be071b3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d933839ef9394df9969e9410f5b8dcc9",
       "max": 100,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1097ab2cbee749658365fd576dd76510",
       "value": 100
      }
     },
     "a11e91858bdb42ebbd50979012b86be5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b0ab429894084814baf5fbc8d7b6107d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b92158aaca674063abb4a57107f2238c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bdaaf2b207bc4a29a47e3ddb626cb7b3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d5a75be943dd4c06b738005bd22926ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d933839ef9394df9969e9410f5b8dcc9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f3b2b95db2464a828e9cfd09341b538d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "fc69a3fd6e4b41a08faca97e92bfc644": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b0ab429894084814baf5fbc8d7b6107d",
       "max": 100,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f3b2b95db2464a828e9cfd09341b538d",
       "value": 100
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
